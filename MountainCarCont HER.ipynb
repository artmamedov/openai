{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1050 Ti'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.utils import shuffle\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd \n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import random\n",
    "from collections import deque\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm as tqdm\n",
    "from statistics import mean, stdev\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.get_device_name(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Box(1,)\n",
      "Observation matrix: Box(2,)\n",
      "Observation min: [-1.2  -0.07]\n",
      "Observation max: [0.6  0.07]\n"
     ]
    }
   ],
   "source": [
    "print(\"Action space:\" , env.action_space) \n",
    "print(\"Observation matrix:\", env.observation_space)\n",
    "print(\"Observation min:\",env.observation_space.low)\n",
    "print(\"Observation max:\", env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(nn.Module):\n",
    "    #Regular Relu except it subtracts by mean\n",
    "    def forward(self, inp): return (inp.clamp_min(0.) - inp.clamp_min(0.).mean())\n",
    "\n",
    "class DeepQ(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        \n",
    "        super(DeepQ, self).__init__()\n",
    "\n",
    "        modules = []\n",
    "        modules.append(nn.Linear(layers[0]+1, layers[1]))\n",
    "        \n",
    "        for i in range(1,len(layers)-1):\n",
    "            modules.append(ReLU())\n",
    "            modules.append(nn.Linear(layers[i], layers[i+1]))\n",
    "            \n",
    "        self.layers = nn.Sequential(*modules)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.layers(x.float())\n",
    "    \n",
    "    \n",
    "    \n",
    "#Class is useful so that I remember the order of appends using shift+tab\n",
    "class Memory():\n",
    "    def __init__(self, maxMemory):\n",
    "        #Deque is a fifo list with a max length\n",
    "        self.memory = deque(maxlen = maxMemory)\n",
    "        \n",
    "    #Passes in Bellman equation parameters\n",
    "    def update(self, goal, state, action, reward, next_state, done):\n",
    "        self.memory.append((goal, state, action, reward, next_state, done))\n",
    "    \n",
    "    #Takes a sample of size batch_size from memory\n",
    "    def sample(self, batch_size):\n",
    "        #Zip into tuples all states, then all actions, then all rewards, etc\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.memory)\n",
    "    \n",
    "def epsilon_decay(step, eps_max, eps_min, eps_decay):\n",
    "    return eps_min + (eps_max - eps_min) * math.exp(-1. * step * eps_decay)\n",
    "\n",
    "def update_model(model,target_model,opt,memory,discount,batch_size):\n",
    "\n",
    "    #Take a sample from memory with batch size\n",
    "    goals, states, actions, rewards, next_states, dones = memory.sample(batch_size)\n",
    "\n",
    "    goals       = torch.tensor(goals ,dtype = torch.float, device = device)  \n",
    "    states      = torch.tensor(states ,dtype = torch.float, device = device)\n",
    "    next_states = torch.tensor(next_states, dtype = torch.float, device = device)\n",
    "    actions     = torch.tensor(actions, dtype = torch.long, device = device).unsqueeze(1)\n",
    "    rewards     = torch.tensor(rewards, dtype = torch.float, device = device)\n",
    "    dones       = torch.tensor(dones, dtype = torch.float, device = device)\n",
    "    \n",
    "    #Take Q from current state based on actions taken at that state\n",
    "    Qs = model(torch.cat([goals,states], dim=1))    \n",
    "    \n",
    "    Qs = Qs.squeeze()\n",
    "        \n",
    "    #Calculate next Q using target model and get the biggest one\n",
    "    \n",
    "    next_Qs = target_model(torch.cat([goals,next_states], dim=1)).max(dim=1)[0]\n",
    "    \n",
    "    #print(next_Qs)\n",
    "    \n",
    "    bellman = rewards + (1-dones) * discount * next_Qs\n",
    "    #print(f\"Bellman: {bellman}\")\n",
    "    \n",
    "    #MSE loss between Bellman from target model and actions Qs from current model\n",
    "    loss = loss_func(bellman,Qs) \n",
    "    \n",
    "    #print(f\"Loss: {loss}\")\n",
    "    \n",
    "    #Gradient \n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def plot(rewards, text):\n",
    "    clear_output(True)\n",
    "    fig = plt.figure(figsize=(20,5))\n",
    "    plt.xlabel(text, ha=\"center\")\n",
    "    plt.title(f\"Rewards, Batch # {len(rewards)}\")\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run(layers, batch_size, discount, eps_max, eps_min, eps_decay, update_freq, lr, memory_size, loss_func, noise_scale):\n",
    "    \n",
    "    goal = np.array([env.goal_position])\n",
    "    low = env.action_space.low[0] \n",
    "    high = env.action_space.high[0]\n",
    "    noise = noise_scale\n",
    "    \n",
    "    model = DeepQ(layers).cuda()\n",
    "    target_model = DeepQ(layers).cuda()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    rewards = []\n",
    "    memory = Memory(int(memory_size))\n",
    "    \n",
    "    max_reward = 0\n",
    "    best_episode = 0\n",
    "    iter = 0\n",
    "    \n",
    "    name = (f\"      Layers: {layers}\"\n",
    "            f\"      Loss function: {loss_func.__name__}\"\n",
    "            f\"      Batch size: {batch_size}\"\n",
    "            f\"      Discount: {discount}\"\n",
    "            f\"      Epsilon: {eps_max} to {eps_min} decay: {eps_decay}\"\n",
    "            f\"      Learning rate: {lr}\"\n",
    "            f\"      Update freq: {update_freq}\"\n",
    "            f\"      Memory size: {memory_size}\")\n",
    "           \n",
    "    for episode in range(1,num_episodes+1):\n",
    "        \n",
    "        state = env.reset()\n",
    "        episode_rewards = 0\n",
    "        done = False\n",
    "\n",
    "        \n",
    "        while not done: #Run until poll falls or env time ends\n",
    "            \n",
    "            #Lets you watch, but trains so much faster if you don't\n",
    "            #env.render()\n",
    "                    \n",
    "            #Calculate Q Value\n",
    "            Q = model(torch.cat([torch.tensor(goal, device=device),torch.tensor(state, device = device)]).unsqueeze(0))\n",
    "                        \n",
    "            epsilon = epsilon_decay(iter, eps_max, eps_min, eps_decay)\n",
    "            \n",
    "            #Do we do take a random action?\n",
    "            if random.random() > epsilon:\n",
    "                action = [torch.clamp(Q + noise, low, high).item()] #Clamp makes sure it's within acceptable range\n",
    "            else:\n",
    "                action = [np.random.uniform(low, high)] #Random action\n",
    "        \n",
    "            #Take step and update memory\n",
    "            next_state, reward, done, info = env.step(action) #reward is {-1,0}\n",
    "            \n",
    "            memory.update(goal, state, action, reward, next_state, done)\n",
    "            memory.update([state[0]],state, action, 0, next_state, True) #HER implementation?\n",
    "\n",
    "            #Updates the model if you can take a proper batch\n",
    "            if len(memory) > batch_size:\n",
    "                loss = update_model(model,target_model,opt,memory,discount,batch_size) #SGD w/ return loss\n",
    "        \n",
    "            #Update target model every update_freq steps:\n",
    "            if iter%update_freq == 0: \n",
    "                target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "            #Updates\n",
    "            state = next_state\n",
    "            episode_rewards += reward\n",
    "            iter += 1\n",
    "\n",
    "            \n",
    "        rewards.append(episode_rewards)    \n",
    "        if episode_rewards > max_reward:\n",
    "            max_reward = episode_rewards\n",
    "            best_episode = episode\n",
    "            \n",
    "        plot(rewards, name)\n",
    "        print(epsilon, max_reward)\n",
    "        \n",
    "        #End condition\n",
    "        if len(rewards)>100:\n",
    "            average = mean(rewards[-100:])\n",
    "            print(average)\n",
    "            if average > -110:\n",
    "                env.close()\n",
    "                print(f\"Finished in {episode} episodes!\")\n",
    "                return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "\n",
    "m = env.observation_space.shape[0]\n",
    "n = 1\n",
    "\n",
    "layers = [m,64,64,64,64,n]\n",
    "\n",
    "loss_func = F.mse_loss\n",
    "\n",
    "batch_size = 128\n",
    "memory_size = 1e6\n",
    "discount = 0.99\n",
    "eps_max = 1\n",
    "eps_min = 1e-3\n",
    "eps_decay = 1e-4\n",
    "update_freq = 5 #How frequently target network gets updated \n",
    "lr = 1e-3\n",
    "noise_scale = 0.01 #Random noise is added to each move\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run(layers,  batch_size, discount, eps_max, eps_min, eps_decay, update_freq, lr, memory_size, loss_func, noise_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
