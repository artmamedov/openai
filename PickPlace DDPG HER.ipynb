{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artur\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1050 Ti'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.utils import shuffle\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd \n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import random\n",
    "from collections import deque\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm as tqdm\n",
    "from statistics import mean, stdev\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.get_device_name(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Box(1,)\n",
      "Observation matrix: Box(3,)\n",
      "Observation min: [-1. -1. -8.]\n",
      "Observation max: [1. 1. 8.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Action space:\" , env.action_space) \n",
    "print(\"Observation matrix:\", env.observation_space)\n",
    "print(\"Observation min:\",env.observation_space.low)\n",
    "print(\"Observation max:\", env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(nn.Module):\n",
    "    #Regular Relu except it subtracts by mean\n",
    "    \n",
    "    def forward(self, inp): return (inp.clamp_min(0.) - inp.clamp_min(0.).mean())\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, layers, goal_size = 0):\n",
    "        \n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        modules = []\n",
    "        modules.append(nn.Linear(layers[0] + goal_size, layers[1])) #+1 from action_size and +1 from goal_dim\n",
    "        \n",
    "        for i in range(1,len(layers)-1):\n",
    "            modules.append(ReLU())\n",
    "            modules.append(nn.Linear(layers[i], layers[i+1]))\n",
    "            \n",
    "        self.layers = nn.Sequential(*modules)\n",
    "        \n",
    "    def forward(self,state, goal):\n",
    "        x = torch.cat([state, goal], dim = -1)\n",
    "        return torch.tanh(self.layers(x.float())) #tanh to bring to -1, 1\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, layers, action_size = 1, goal_size = 0):\n",
    "\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        modules = []\n",
    "        modules.append(nn.Linear(layers[0]+action_size + goal_size, layers[1])) #+1 from action_size and +1 from goal_dim\n",
    "        \n",
    "        for i in range(1,len(layers)-1):\n",
    "            modules.append(ReLU())\n",
    "            modules.append(nn.Linear(layers[i], layers[i+1]))\n",
    "            \n",
    "        self.layers = nn.Sequential(*modules)\n",
    "        \n",
    "    def forward(self, state, action, goal = None):\n",
    "        if goal is not None: x = torch.cat((state, action, goal), dim = 1)\n",
    "        else: x = torch.cat((state, action), dim = 1)\n",
    "\n",
    "        return self.layers(x)\n",
    "    \n",
    "    \n",
    "#Class is useful so that I remember the order of appends using shift+tab\n",
    "class Memory():\n",
    "    def __init__(self, maxMemory):\n",
    "        #Deque is a fifo list with a max length\n",
    "        self.memory = deque(maxlen = maxMemory)\n",
    "        \n",
    "    #Passes in Bellman equation parameters\n",
    "    def update(self, goal, state, action, reward, next_state, done):\n",
    "        self.memory.append((goal, state, action, reward, next_state, done))\n",
    "    \n",
    "    #Takes a sample of size batch_size from memory\n",
    "    def sample(self, batch_size):\n",
    "        #Zip into tuples all states, then all actions, then all rewards, etc\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.memory)\n",
    "    \n",
    "def epsilon_decay(step, eps_max, eps_min, eps_decay):\n",
    "    return eps_min + (eps_max - eps_min) * math.exp(-1. * step * eps_decay)\n",
    "\n",
    "def update_model(model,target_model,model_opt, critic,target_critic, critic_opt,memory,discount,batch_size):\n",
    "\n",
    "    #Take a sample from memory with batch size\n",
    "    goals, states, actions, rewards, next_states, dones = memory.sample(batch_size)\n",
    "    \n",
    "    goals       = torch.tensor(goals ,dtype = torch.float, device = device)  \n",
    "    states      = torch.tensor(states ,dtype = torch.float, device = device)\n",
    "    next_states = torch.tensor(next_states, dtype = torch.float, device = device)\n",
    "    actions     = torch.tensor(actions, dtype = torch.long, device = device)\n",
    "    rewards     = torch.tensor(rewards, dtype = torch.float, device = device)\n",
    "    dones       = torch.tensor(dones, dtype = torch.float, device = device)\n",
    "\n",
    "    next_actions = target_model(next_states, goals)\n",
    "    next_Qs = target_critic(next_states, next_actions, goals)\n",
    "    \n",
    "    bellman = rewards + (1-dones) * discount * next_Qs\n",
    "    expected_Qs = critic(states, actions, goals)\n",
    "    critic_loss = loss_func(bellman,expected_Qs) \n",
    "        \n",
    "    #Critic Gradient \n",
    "    critic_loss.backward()\n",
    "    critic_opt.step()\n",
    "    critic_opt.zero_grad()\n",
    "\n",
    "    actions_pred = model(states, goals)\n",
    "    actor_loss = -critic(states, actions_pred, goals).mean()\n",
    "    # Minimize the loss\n",
    "    actor_loss.backward()\n",
    "    model_opt.step()\n",
    "    model_opt.zero_grad()\n",
    "\n",
    "\n",
    "def plot(rewards, text):\n",
    "    clear_output(True)\n",
    "    fig = plt.figure(figsize=(20,5))\n",
    "    plt.xlabel(text, ha=\"center\")\n",
    "    plt.title(f\"Rewards, Batch # {len(rewards)}\")\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run(layers, batch_size, discount, eps_max, eps_min, eps_decay, update_freq, lr, memory_size, loss_func, noise_scale):\n",
    "    \n",
    "    goal = np.array([env.goal_position])\n",
    "    low = env.action_space.low[0] \n",
    "    high = env.action_space.high[0]\n",
    "    noise = noise_scale\n",
    "    \n",
    "\n",
    "    model = Actor(layers, goal_size = len(goal)).cuda()\n",
    "    target_model = Actor(layers, goal_size = len(goal)).cuda()\n",
    "    \n",
    "    critic = Critic(layers, action_size = 1, goal_size = len(goal)).cuda()\n",
    "    target_critic = Critic(layers, action_size = 1, goal_size = len(goal)).cuda()\n",
    "    \n",
    "    model_opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    critic_opt = optim.Adam(critic.parameters(), lr=lr)\n",
    "    \n",
    "    rewards = []\n",
    "    memory = Memory(int(memory_size))\n",
    "    \n",
    "    max_reward = 0\n",
    "    best_episode = 0\n",
    "    iter = 0\n",
    "    \n",
    "    name = (f\"      Layers: {layers}\"\n",
    "            f\"      Loss function: {loss_func.__name__}\"\n",
    "            f\"      Batch size: {batch_size}\"\n",
    "            f\"      Discount: {discount}\"\n",
    "            f\"      Epsilon: {eps_max} to {eps_min} decay: {eps_decay}\"\n",
    "            f\"      Learning rate: {lr}\"\n",
    "            f\"      Update freq: {update_freq}\"\n",
    "            f\"      Memory size: {memory_size}\")\n",
    "           \n",
    "    for episode in range(1,num_episodes+1):\n",
    "        \n",
    "        state = env.reset()\n",
    "        episode_rewards = 0\n",
    "        done = False\n",
    "\n",
    "        \n",
    "        while not done: #Run until poll falls or env time ends\n",
    "            \n",
    "            #Lets you watch, but trains so much faster if you don't\n",
    "            #env.render()\n",
    "                    \n",
    "            #Calculate Q Value\n",
    "            Q = model(torch.tensor(state, device = device),torch.tensor(goal, device=device))\n",
    "            \n",
    "            epsilon = epsilon_decay(iter, eps_max, eps_min, eps_decay)\n",
    "            \n",
    "            #Do we do take a random action?\n",
    "            if random.random() > epsilon:\n",
    "                action = [torch.clamp(Q + noise, low, high).item()] #Clamp makes sure it's within acceptable range\n",
    "            else:\n",
    "                action = [np.random.uniform(low, high)] #Random action\n",
    "        \n",
    "            #Take step and update memory\n",
    "            next_state, reward, done, info = env.step(action) #reward is {-1,0}\n",
    "            memory.update(goal, state, action, reward, next_state, done)\n",
    "            memory.update([state[0]],state, action, 100, next_state, True) #HER implementation\n",
    "\n",
    "            #Updates the model if you can take a proper batch\n",
    "            if len(memory) > batch_size:\n",
    "                update_model(model,target_model,model_opt, critic,target_critic, critic_opt,memory,discount,batch_size) #SGD w/ return loss\n",
    "        \n",
    "            #Update target model every update_freq steps:\n",
    "            if iter%update_freq == 0: \n",
    "                target_model.load_state_dict(model.state_dict())\n",
    "                target_critic.load_state_dict(critic.state_dict())\n",
    "\n",
    "            #Updates\n",
    "            state = next_state\n",
    "            episode_rewards += reward\n",
    "            iter += 1\n",
    "\n",
    "            \n",
    "        rewards.append(episode_rewards)    \n",
    "        if episode_rewards > max_reward:\n",
    "            max_reward = episode_rewards\n",
    "            best_episode = episode\n",
    "            \n",
    "        plot(rewards, name)\n",
    "        print(epsilon, max_reward)\n",
    "        \n",
    "        #End condition\n",
    "        if len(rewards)>100:\n",
    "            average = mean(rewards[-100:])\n",
    "            print(average)\n",
    "            if average > 0:\n",
    "                env.close()\n",
    "                print(f\"Finished in {episode} episodes!\")\n",
    "                return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "\n",
    "m = env.observation_space.shape[0]\n",
    "n = 1\n",
    "\n",
    "layers = [m,256,256,n]\n",
    "\n",
    "loss_func = F.mse_loss\n",
    "\n",
    "batch_size = 128\n",
    "memory_size = 1e6\n",
    "discount = 0.99\n",
    "eps_max = 1\n",
    "eps_min = 1e-4\n",
    "eps_decay = 1e-4\n",
    "update_freq = 1 #How frequently target network gets updated \n",
    "lr = 1e-3\n",
    "noise= 0.01 #Random noise is added to each move\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PendulumEnv' object has no attribute 'goal_position'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5c1170f4eea7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_decay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-25f0d5d9bbb4>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(layers, batch_size, discount, eps_max, eps_min, eps_decay, update_freq, lr, memory_size, loss_func, noise_scale)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_decay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise_scale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mgoal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgoal_position\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mlow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mhigh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"attempted to get missing private attribute '{}'\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PendulumEnv' object has no attribute 'goal_position'"
     ]
    }
   ],
   "source": [
    "results = run(layers,  batch_size, discount, eps_max, eps_min, eps_decay, update_freq, lr, memory_size, loss_func, noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
